{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:25:30.600477: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 13:25:30.747557: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-02 13:25:31.696875: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 13:25:31.696946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 13:25:31.696953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fede86c8db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import tqdm\n",
    "from object_detection_data import ObjectDetectionDataset, Encoder, TransformedObjectDetectionDataset\n",
    "import torch.nn as nn\n",
    "from object_detection_models import MultiClassJetNet, LightningMultiClassJetNet\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "torch.manual_seed(2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Select the correct scalings for the default maps\n",
    "- Select the correct initial learning rate\n",
    "- Add learning rate schedule\n",
    "- Log additional metrics\n",
    "- Benchmark training loop\n",
    "- Add validation data set\n",
    "- Show Network predictions for example images\n",
    "- Add data augmentation\n",
    "- Make use of torch compile?\n",
    "- Refactor the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scalings = [torch.tensor(\n",
    "    [0.25, 0.25]), torch.tensor(\n",
    "    [0.125, 0.125]), torch.tensor([0.125 / 2, 0.125 / 2]), torch.tensor([0.125 / 4, 0.125 / 4])]\n",
    "feature_map_size = (8, 10)\n",
    "num_classes = 4\n",
    "encoder = Encoder(default_scalings, feature_map_size, num_classes)\n",
    "data_path = 'SPLObjDetectDatasetV2/train'\n",
    "transformed_train_data = TransformedObjectDetectionDataset(\n",
    "    'SPLObjDetectDatasetV2/train', encoder)\n",
    "transformed_val_data = TransformedObjectDetectionDataset(\n",
    "    'SPLObjDetectDatasetV2/val', encoder)\n",
    "train_loader = DataLoader(transformed_train_data, batch_size=16,\n",
    "                          shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(transformed_val_data, batch_size=16,\n",
    "                        shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ MultiClassJetNet │ 27.1 K │\n",
       "└───┴───────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ MultiClassJetNet │ 27.1 K │\n",
       "└───┴───────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 27.1 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 27.1 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 27.1 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 27.1 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969d2f3d8c4f417faa65c0b27cc4435b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\r\u001b[2K/home/jonathan/hulks/ml/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:48\n",
       "8: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you \n",
       "turn shuffling off for val/test/predict dataloaders.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\r\u001b[2K/home/jonathan/hulks/ml/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:48\n",
       "8: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you \n",
       "turn shuffling off for val/test/predict dataloaders.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl_model = LightningMultiClassJetNet(num_classes, len(default_scalings))\n",
    "trainer = pl.Trainer(limit_predict_batches=100,\n",
    "                     max_epochs=10, callbacks=RichProgressBar())\n",
    "trainer.fit(model=pl_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms = T.Compose([T.PILToTensor(), T.ConvertImageDtype(\n",
    "    torch.float32), T.Normalize([0.3333, 0.4772, 0.3155], [0.0905, 0.1207, 0.1082]), T.Grayscale(), T.Resize((60, 80))])\n",
    "\n",
    "default_scalings = [torch.tensor(\n",
    "    [0.25, 0.25]), torch.tensor(\n",
    "    [0.125, 0.125]), torch.tensor([0.125 / 2, 0.125 / 2]), torch.tensor([0.125 / 4, 0.125 / 4])]\n",
    "feature_map_size = (8, 10)\n",
    "num_classes = 4\n",
    "encoder = Encoder(default_scalings, feature_map_size, num_classes)\n",
    "train_data_path = 'SPLObjDetectDatasetV2/train'\n",
    "val_data_path = 'SPLObjDetectDatasetV2/val'\n",
    "train_data = ObjectDetectionDataset(\n",
    "    train_data_path, encoder, transforms=transforms)\n",
    "validation_data = ObjectDetectionDataset(\n",
    "    val_data_path, encoder, transforms=transforms)\n",
    "train_loader = DataLoader(train_data, batch_size=16,\n",
    "                          shuffle=True, num_workers=6)\n",
    "val_loader = DataLoader(validation_data, batch_size=16,\n",
    "                          shuffle=False, num_workers=6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8179af3a6648109ce06e2d9f5db23fa843e75289adf6dc335b42c828d3bf7f76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
