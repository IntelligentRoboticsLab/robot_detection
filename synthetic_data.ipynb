{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import torchvision.transforms.functional as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from object_detection_data import Encoder\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "%aimport \n",
    "from object_detection_models import MultiClassJetNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticData(DataLoader):\n",
    "    def __init__(self, image_width, image_height, length, encoder):\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.length = length\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.images = []\n",
    "        self.encoded_bounding_boxes = []\n",
    "        self.encoded_target_classes = []\n",
    "        self.target_masks = []\n",
    "        for _ in range(length):\n",
    "            image, bounding_box = self._generate_image()\n",
    "            encoded_bounding_boxes, target_mask, target_classes = self.encoder.apply(\n",
    "                bounding_box, torch.tensor([[1]])\n",
    "            )\n",
    "            self.images.append(image)\n",
    "            self.encoded_bounding_boxes.append(encoded_bounding_boxes)\n",
    "            self.encoded_target_classes.append(target_classes)\n",
    "            self.target_masks.append(target_mask)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.images[idx],\n",
    "            self.encoded_bounding_boxes[idx],\n",
    "            self.target_masks[idx],\n",
    "            self.encoded_target_classes[idx],\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def _generate_image(self):\n",
    "        image = Image.new(\"1\", (image_width, image_height))\n",
    "        image_draw = ImageDraw.Draw(image)\n",
    "        center = torch.tensor(\n",
    "            [\n",
    "                torch.randint(0, image_width - 1, (1,)).item(),\n",
    "                torch.randint(0, image_height - 1, (1,)).item(),\n",
    "            ]\n",
    "        )\n",
    "        size = torch.tensor([image_width * 0.25, image_height * 0.25])\n",
    "        upper_left = center - size / 2\n",
    "        lower_right = center + size / 2\n",
    "        image_draw.rectangle(\n",
    "            [upper_left[0], upper_left[1], lower_right[0], lower_right[1]], fill=255\n",
    "        )\n",
    "        bounding_box = torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    center[0].item() / image_width,\n",
    "                    center[1].item() / image_height,\n",
    "                    size[0].item() / image_width,\n",
    "                    size[1].item() / image_height,\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "        return T.pil_to_tensor(image).to(torch.float), bounding_box\n",
    "\n",
    "\n",
    "image_width = 80\n",
    "image_height = 60\n",
    "torch.manual_seed(2)\n",
    "\n",
    "default_scalings = torch.tensor([[0.25, 0.25]])\n",
    "num_classes = 1\n",
    "encoder = Encoder(default_scalings, num_classes)\n",
    "\n",
    "data = SyntheticData(image_width, image_height, 10000, encoder)\n",
    "data_loader = DataLoader(data, batch_size=64, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jonathan/hulks/ml/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ accuracy            │ MulticlassAccuracy │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ input_layer         │ NormConv2dReLU     │    146 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ depth_wise_backbone │ Sequential         │  4.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ classifier          │ Sequential         │ 20.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ output_layer        │ Conv2d             │    150 │\n",
       "└───┴─────────────────────┴────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ accuracy            │ MulticlassAccuracy │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ input_layer         │ NormConv2dReLU     │    146 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ depth_wise_backbone │ Sequential         │  4.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ classifier          │ Sequential         │ 20.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ output_layer        │ Conv2d             │    150 │\n",
       "└───┴─────────────────────┴────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 26.1 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 26.1 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 26.1 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 26.1 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/hulks/ml/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f932d7d511c84fa58b8eb3122f042592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"synthetic_data_logs/\")\n",
    "\n",
    "\n",
    "pl_model = MultiClassJetNet(encoder, 1e-3)\n",
    "trainer = pl.Trainer(\n",
    "    limit_predict_batches=100, max_epochs=60, callbacks=[RichProgressBar()], logger = tb_logger\n",
    ")\n",
    "trainer.fit(model=pl_model, train_dataloaders=data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from object_detection_data import calc_iou_tensor\n",
    "\n",
    "\n",
    "def xywh_to_tlbr(xywh_bounding_boxes):\n",
    "    # bounding boxes in format (batch, 4) (cx, cy, w, h)\n",
    "    tlbr_bounding_boxes = torch.zeros_like(xywh_bounding_boxes)\n",
    "    tlbr_bounding_boxes[:, 0:2] = (\n",
    "        xywh_bounding_boxes[:, 0:2] - xywh_bounding_boxes[:, 2:4] / 2\n",
    "    )\n",
    "\n",
    "    tlbr_bounding_boxes[:, 2:4] = (\n",
    "        xywh_bounding_boxes[:, 0:2] + xywh_bounding_boxes[:, 2:4] / 2\n",
    "    )\n",
    "    return tlbr_bounding_boxes\n",
    "\n",
    "\n",
    "def decode_model_output(encoder, predicted_boxes):\n",
    "    decoded_boxes = torch.zeros(predicted_boxes.shape)\n",
    "    decoded_boxes[:, :, 0:2] = encoder.default_boxes_xy_wh[:, 2:4] * (\n",
    "        encoder.default_boxes_xy_wh[:, 0:2] - predicted_boxes[:, :, 0:2]\n",
    "    )\n",
    "    decoded_boxes[:, :, 2:4] = encoder.default_boxes_xy_wh[:, 2:4] * torch.exp(\n",
    "        predicted_boxes[:, :, 2:4]\n",
    "    )\n",
    "    # decoded_boxes = decoded_boxes.squeeze()\n",
    "    return decoded_boxes\n",
    "\n",
    "\n",
    "# Raw model output\n",
    "# -> Filter detected objects\n",
    "# -> Calculate overlap of prediceted bounding boxes with groundtruth bounding boxes\n",
    "# -> Filter all boxes that meet a threshold\n",
    "# -> Calculate precision\n",
    "pl_model = MultiClassJetNet(encoder, 1e-3)\n",
    "for batch in data_loader:\n",
    "    image, target_boxes, target_masks, target_classes = batch\n",
    "    predicted_boxes, predicted_class_logits = pl_model(image)\n",
    "    class_probabilities = F.softmax(predicted_class_logits, dim=-1)\n",
    "    decoded_boxes = decode_model_output(encoder, predicted_boxes).reshape((-1, 4))\n",
    "    detection_mask = torch.argmax(class_probabilities, dim=-1) > 0\n",
    "    # (cx, cy, w, h) -> (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "    object_boxes_tlbr = xywh_to_tlbr(decoded_boxes[detection_mask])\n",
    "    target_boxes_tlbr = xywh_to_tlbr(target_boxes[target_masks])\n",
    "    ious = calc_iou_tensor(object_boxes_tlbr, target_boxes_tlbr)\n",
    "    break\n",
    "ious[0] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5120, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=400x300>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%aimport visualize\n",
    "from visualize import draw_model_output, image_grid\n",
    "\n",
    "grid_size = 5\n",
    "image_list = []\n",
    "for i in range(grid_size * grid_size):\n",
    "    image, encoded_bounding_boxes, target_masks, encoded_target_classes, idx = data[i]\n",
    "    predicted_boxes, predicted_class_logits = pl_model(image.unsqueeze(0))\n",
    "    image_list.append(\n",
    "        draw_model_output(\n",
    "            image,\n",
    "            predicted_boxes,\n",
    "            predicted_class_logits,\n",
    "            torch.tensor([0, 1]),\n",
    "            encoder,\n",
    "        )\n",
    "    )\n",
    "image_grid(image_list, grid_size, grid_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8179af3a6648109ce06e2d9f5db23fa843e75289adf6dc335b42c828d3bf7f76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
