{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pipeline branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Encoder\n",
    "import utils\n",
    "import torch\n",
    "from datasets import RoboEireanDataModule\n",
    "import lightning.pytorch as pl\n",
    "from PIL import ImageDraw, Image\n",
    "import os\n",
    "\n",
    "from models import JetNet, SingleShotDetector, ObjectDetectionTask\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from visualize import draw_bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_bounding_boxes(image, bounding_boxes):\n",
    "    # Open the image\\\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Iterate over the bounding boxes\n",
    "    for bbox in bounding_boxes:\n",
    "        # Extract the box coordinates\n",
    "        x_center, y_center, box_width, box_height = bbox\n",
    "        \n",
    "        # Convert YOLO coordinates to absolute coordinates\n",
    "        x_min = (x_center - box_width / 2) * width\n",
    "        y_min = (y_center - box_height / 2) * height\n",
    "        x_max = (x_center + box_width / 2) * width\n",
    "        y_max = (y_center + box_height / 2) * height\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        draw.rectangle([(x_min, y_min), (x_max, y_max)], outline=\"red\")\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model checkpoint and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-1\n",
    "ALPHA = 2.0\n",
    "NUM_CLASSES = 1\n",
    "DEFAULT_SCALINGS = torch.tensor(\n",
    "    [\n",
    "        [0.06549374, 0.12928654],\n",
    "        [0.11965626, 0.26605093],\n",
    "        [0.20708716, 0.38876095],\n",
    "        [0.31018215, 0.47485098],\n",
    "        [0.415882, 0.8048184],\n",
    "        [0.7293086, 0.8216225],\n",
    "    ]\n",
    ")\n",
    "encoder = Encoder(DEFAULT_SCALINGS, NUM_CLASSES)\n",
    "model = JetNet(NUM_CLASSES, DEFAULT_SCALINGS.shape[0])\n",
    "loss = SingleShotDetector(ALPHA)\n",
    "\n",
    "version = 22\n",
    "\n",
    "checkpoint = os.listdir(f\"new_logs/lightning_logs/version_{version}/checkpoints\")[0]\n",
    "checkpoint_path = f\"new_logs/lightning_logs/version_{version}/checkpoints/{checkpoint}\"\n",
    "\n",
    "loaded_model = ObjectDetectionTask.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint_path, \n",
    "    model=model, \n",
    "    loss=loss, \n",
    "    encoder=encoder, \n",
    "    learning_rate=LEARNING_RATE)\n",
    "loaded_model\n",
    "\n",
    "# get the data we want to visualize and predict on\n",
    "data_module = RoboEireanDataModule(\"data/raw/\", encoder, 128)\n",
    "data_module.setup(\"fit\")  # TODO: inspect different stages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over all data and check for positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, image_tensor, target_bb, target_class, image_paths in data_module.val_dataloader():\n",
    "\n",
    "    \n",
    "    predicted_boxes, predicted_logits = loaded_model.model(image)\n",
    "\n",
    "    predicted_classes, softmax = utils.calculate_predicted_classes(predicted_logits)\n",
    "    sorted_softmax = torch.sort(softmax[0][:,1], descending=True).indices\n",
    "\n",
    "    decoded_boxes = encoder.decode(predicted_boxes).squeeze()\n",
    "\n",
    "    for i in range(len(image)):\n",
    "        \n",
    "        image_pil = T.ToPILImage()(image[i])\n",
    "\n",
    "        if len(decoded_boxes[i][predicted_classes[i] > 0]) > 0:\n",
    "            draw = draw_bounding_boxes(image_pil, decoded_boxes[i][predicted_classes[i] > 0])\n",
    "            print(image_paths[i])\n",
    "            plt.imshow(image_pil)\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicate prediction logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_count = 0\n",
    "\n",
    "for i in range(127):\n",
    "    for j in range(i + 1, 128):\n",
    "        print(predicted_logits[i])\n",
    "        print(predicted_logits[j])\n",
    "        if not torch.all(predicted_logits[i].eq(predicted_logits[j])):\n",
    "\n",
    "            equal_count += 1\n",
    "\n",
    "print(f'{equal_count} of the 128 predictions are not equal to each other')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import TransformedRoboEireanData, RoboEireanDataWithEncoder\n",
    "import utils\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "import pytorch_lightning as pl\n",
    "from models import MultiClassJetNet\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from PIL import ImageDraw, Image\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from visualize import draw_bounding_box\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw RoboEirean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default scalings for the default boxes determined by k-means clustering\n",
    "default_box_scalings = torch.tensor(\n",
    "    [\n",
    "        [0.06549374, 0.12928654],\n",
    "        [0.11965626, 0.26605093],\n",
    "        [0.20708716, 0.38876095],\n",
    "        [0.31018215, 0.47485098],\n",
    "        [0.415882, 0.8048184],\n",
    "        [0.7293086, 0.8216225],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "classes = [\"robot\"]\n",
    "encoder = utils.Encoder(default_box_scalings, [\"robot\"])\n",
    "\n",
    "image_transforms = T.Compose(\n",
    "            [\n",
    "                T.Grayscale(),\n",
    "                T.PILToTensor(),\n",
    "                T.ConvertImageDtype(torch.float32),\n",
    "                T.Resize((60, 80)),\n",
    "            ]\n",
    "        )\n",
    "bounding_box_transforms = T.Compose([])\n",
    "\n",
    "raw_train_data = RoboEireanDataWithEncoder(os.path.join(\"data\", \"raw\", \"train\"),\n",
    "                                           encoder,\n",
    "                                           [\"robot\"], image_transforms=image_transforms, bounding_box_transforms=bounding_box_transforms)\n",
    "\n",
    "raw_val_data = RoboEireanDataWithEncoder(os.path.join(\"data\", \"raw\", \"val\"),\n",
    "                                           encoder,\n",
    "                                           [\"robot\"], image_transforms=image_transforms, bounding_box_transforms=bounding_box_transforms)\n",
    "train_loader = DataLoader(\n",
    "    raw_train_data, batch_size=32, shuffle=True, num_workers=1\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    raw_val_data, batch_size=32, shuffle=False, num_workers=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Checkpoint and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-3\n",
    "version = 28\n",
    "\n",
    "checkpoint = os.listdir(f\"lightning_logs/version_{version}/checkpoints\")[0]\n",
    "trained_model = MultiClassJetNet.load_from_checkpoint(f\"lightning_logs/version_{version}/checkpoints/{checkpoint}\", num_classes=len(classes), num_boxes=6, learning_rate=learning_rate)\n",
    "trained_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Draw Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_bounding_boxes(image, bounding_boxes):\n",
    "    # Open the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Iterate over the bounding boxes\n",
    "    for bbox in bounding_boxes:\n",
    "        # Extract the box coordinates\n",
    "        x_center, y_center, box_width, box_height = bbox\n",
    "        \n",
    "        # Convert YOLO coordinates to absolute coordinates\n",
    "        x_min = (x_center - box_width / 2) * width\n",
    "        y_min = (y_center - box_height / 2) * height\n",
    "        x_max = (x_center + box_width / 2) * width\n",
    "        y_max = (y_center + box_height / 2) * height\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        draw.rectangle([(x_min, y_min), (x_max, y_max)], outline=\"red\")\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Positive Predictions and Respective Raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, encoded_bounding_boxes, target_mask, target_classes, image_paths in val_loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_boxes, predicted_logits = trained_model(images)\n",
    "\n",
    "    predicted_classes, softmax = utils.calculate_predicted_classes(predicted_logits)\n",
    "\n",
    "    decoded_boxes, encoded_predicted_classes, is_object = encoder.decode_model_output(predicted_boxes, predicted_classes)\n",
    "\n",
    "    decoded_boxes = decoded_boxes.view(len(images), 480, 4)\n",
    "    is_object = is_object.view(len(images), 480)\n",
    "    encoded_predicted_classes = encoded_predicted_classes.view(len(images), 480)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        image_pil = T.ToPILImage()(images[i][0]).convert('RGB')\n",
    "\n",
    "        if len(decoded_boxes[i][is_object[i]]) > 0:\n",
    "\n",
    "            print(image_paths[i])\n",
    "            draw = draw_bounding_boxes(image_pil, decoded_boxes[i][is_object[i]])\n",
    "\n",
    "            raw_image = Image.open(image_paths[i])\n",
    "\n",
    "            plt.imshow(raw_image)\n",
    "            plt.show()\n",
    "            plt.imshow(image_pil)\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
